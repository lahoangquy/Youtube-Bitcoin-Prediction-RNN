{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin prediction [2022].ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_dEutJxIa14",
        "outputId": "bb11db09-c5d5-401a-b441-0bcff7c8b9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMUtl8_rIgPz",
        "outputId": "4ce4f3f5-9369-4627-9c30-b2e2754de8d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "oArYEw83Mvmi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general parameters for our task are the future horizon of our prediction and the hyperparameter for the network"
      ],
      "metadata": {
        "id": "hg_SFF-YNR45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_days = 30\n",
        "nof_units = 4"
      ],
      "metadata": {
        "id": "BCRyABhUNZ07"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will encapsulate our model creation step in a function. It will accept a single parameter, units, which is the dimension of the innter cells in LSTM"
      ],
      "metadata": {
        "id": "GampQ0c_Ng4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(nunits):\n",
        "\n",
        "    # Initialising the RNN\n",
        "    regressor = Sequential()\n",
        "\n",
        "    # Adding the input layer and the LSTM layer\n",
        "    regressor.add(LSTM(units = nunits, activation = 'sigmoid', input_shape = (None, 1)))\n",
        "\n",
        "    # Adding the output layer\n",
        "    regressor.add(Dense(units = 1))\n",
        "\n",
        "    # Compiling the RNN\n",
        "    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    \n",
        "    return regressor"
      ],
      "metadata": {
        "id": "Fa6DKfSnNfHz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data and encode the date\n",
        "df = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
        "df['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\n",
        "group = df.groupby('date')\n",
        "Real_Price = group['Weighted_Price'].mean()\n",
        "# We will predict the average daily price that is why we need to group the date"
      ],
      "metadata": {
        "id": "mG6DG0fJPfgv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step will be splitting the data into training and testing dataset\n",
        "df_train= Real_Price[:len(Real_Price)-prediction_days] # This will start the dataset at 0 until len(Real_price)-prediction_days\n",
        "df_test= Real_Price[len(Real_Price)-prediction_days:] # This will start the dataset at  len(Real_price)-prediction_days until the end"
      ],
      "metadata": {
        "id": "Ww8Vte4lQgAn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform preprocessing\n",
        "training_set = df_train.values\n",
        "training_set = np.reshape(training_set, (len(training_set), 1))\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "training_set = sc.fit_transform(training_set)\n",
        "X_train = training_set[0:len(training_set)-1]  # training set\n",
        "y_train = training_set[1:len(training_set)] # testing dataset\n",
        "X_train = np.reshape(X_train, (len(X_train), 1, 1))"
      ],
      "metadata": {
        "id": "7WC_Oh3nRgMd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "\n",
        "# Initialising the RNN\n",
        "regressor = create_model(nunits = 4)\n",
        "# Fitting the RNN to the Training set\n",
        "regressor.fit(X_train, y_train, batch_size = 5, epochs = 100) # We will run for a 100 times"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUoOxz8PSO2m",
        "outputId": "2058cfd7-fb91-4504-ae6e-cd045813bd8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "669/669 [==============================] - 2s 2ms/step - loss: 0.0108\n",
            "Epoch 2/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 0.0069\n",
            "Epoch 3/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Epoch 4/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Epoch 5/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 8.2680e-04\n",
            "Epoch 6/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 2.4661e-04\n",
            "Epoch 7/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 1.0196e-04\n",
            "Epoch 8/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 8.0572e-05\n",
            "Epoch 9/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 7.5712e-05\n",
            "Epoch 10/100\n",
            "669/669 [==============================] - 2s 3ms/step - loss: 7.2738e-05\n",
            "Epoch 11/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 6.6466e-05\n",
            "Epoch 12/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 6.2826e-05\n",
            "Epoch 13/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 5.8315e-05\n",
            "Epoch 14/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 5.5522e-05\n",
            "Epoch 15/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 5.1940e-05\n",
            "Epoch 16/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.9582e-05\n",
            "Epoch 17/100\n",
            "669/669 [==============================] - 2s 2ms/step - loss: 4.8616e-05\n",
            "Epoch 18/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.7040e-05\n",
            "Epoch 19/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.5264e-05\n",
            "Epoch 20/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.5738e-05\n",
            "Epoch 21/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.3235e-05\n",
            "Epoch 22/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.3401e-05\n",
            "Epoch 23/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.1533e-05\n",
            "Epoch 24/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.1238e-05\n",
            "Epoch 25/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.1677e-05\n",
            "Epoch 26/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.0048e-05\n",
            "Epoch 27/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9882e-05\n",
            "Epoch 28/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.0831e-05\n",
            "Epoch 29/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.0862e-05\n",
            "Epoch 30/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9988e-05\n",
            "Epoch 31/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8747e-05\n",
            "Epoch 32/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8743e-05\n",
            "Epoch 33/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9858e-05\n",
            "Epoch 34/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9260e-05\n",
            "Epoch 35/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7706e-05\n",
            "Epoch 36/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8986e-05\n",
            "Epoch 37/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9636e-05\n",
            "Epoch 38/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9258e-05\n",
            "Epoch 39/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7876e-05\n",
            "Epoch 40/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7970e-05\n",
            "Epoch 41/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8723e-05\n",
            "Epoch 42/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7522e-05\n",
            "Epoch 43/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8036e-05\n",
            "Epoch 44/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8537e-05\n",
            "Epoch 45/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9371e-05\n",
            "Epoch 46/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8808e-05\n",
            "Epoch 47/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8117e-05\n",
            "Epoch 48/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7799e-05\n",
            "Epoch 49/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8918e-05\n",
            "Epoch 50/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9778e-05\n",
            "Epoch 51/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8789e-05\n",
            "Epoch 52/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8548e-05\n",
            "Epoch 53/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9603e-05\n",
            "Epoch 54/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8429e-05\n",
            "Epoch 55/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8807e-05\n",
            "Epoch 56/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.0489e-05\n",
            "Epoch 57/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9485e-05\n",
            "Epoch 58/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7824e-05\n",
            "Epoch 59/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7344e-05\n",
            "Epoch 60/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9252e-05\n",
            "Epoch 61/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7747e-05\n",
            "Epoch 62/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7487e-05\n",
            "Epoch 63/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9089e-05\n",
            "Epoch 64/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9334e-05\n",
            "Epoch 65/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 4.0129e-05\n",
            "Epoch 66/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7498e-05\n",
            "Epoch 67/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7908e-05\n",
            "Epoch 68/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9988e-05\n",
            "Epoch 69/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9515e-05\n",
            "Epoch 70/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8658e-05\n",
            "Epoch 71/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9274e-05\n",
            "Epoch 72/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8514e-05\n",
            "Epoch 73/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9283e-05\n",
            "Epoch 74/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7652e-05\n",
            "Epoch 75/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8216e-05\n",
            "Epoch 76/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8340e-05\n",
            "Epoch 77/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9189e-05\n",
            "Epoch 78/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9169e-05\n",
            "Epoch 79/100\n",
            "669/669 [==============================] - 2s 2ms/step - loss: 3.9073e-05\n",
            "Epoch 80/100\n",
            "669/669 [==============================] - 2s 2ms/step - loss: 3.6959e-05\n",
            "Epoch 81/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8998e-05\n",
            "Epoch 82/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9642e-05\n",
            "Epoch 83/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7696e-05\n",
            "Epoch 84/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8318e-05\n",
            "Epoch 85/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8639e-05\n",
            "Epoch 86/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8007e-05\n",
            "Epoch 87/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7713e-05\n",
            "Epoch 88/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8049e-05\n",
            "Epoch 89/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8535e-05\n",
            "Epoch 90/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8808e-05\n",
            "Epoch 91/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8789e-05\n",
            "Epoch 92/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8582e-05\n",
            "Epoch 93/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8142e-05\n",
            "Epoch 94/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9097e-05\n",
            "Epoch 95/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.7578e-05\n",
            "Epoch 96/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8141e-05\n",
            "Epoch 97/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8262e-05\n",
            "Epoch 98/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.9309e-05\n",
            "Epoch 99/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8448e-05\n",
            "Epoch 100/100\n",
            "669/669 [==============================] - 1s 2ms/step - loss: 3.8058e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8120e3f490>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, once the number of epochs are increased, the loss is decreased. That means the model is learning good with high accuracy."
      ],
      "metadata": {
        "id": "5srOQJaOThzY"
      }
    }
  ]
}